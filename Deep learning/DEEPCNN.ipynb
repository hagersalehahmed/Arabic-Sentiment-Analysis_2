{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CETtDDAAETDH",
    "outputId": "984a1828-9162-468c-97af-b46a2d7d9772"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (1.1.2)\n",
      "Requirement already satisfied: ipython in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from keras-tuner) (7.33.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from keras-tuner) (1.22.4)\n",
      "Requirement already satisfied: kt-legacy in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from keras-tuner) (1.0.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from keras-tuner) (21.3)\n",
      "Requirement already satisfied: tensorboard in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from keras-tuner) (2.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from keras-tuner) (2.28.1)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (0.1.3)\n",
      "Requirement already satisfied: pygments in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (2.11.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (3.0.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (0.4.4)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (0.18.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: pickleshare in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (0.7.5)\n",
      "Requirement already satisfied: setuptools>=18.5 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from ipython->keras-tuner) (62.6.0)\n",
      "Requirement already satisfied: decorator in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (5.1.1)\n",
      "Requirement already satisfied: backcall in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from ipython->keras-tuner) (0.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from packaging->keras-tuner) (3.0.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from requests->keras-tuner) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from requests->keras-tuner) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from requests->keras-tuner) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from requests->keras-tuner) (2022.6.15)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (2.9.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (3.3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (0.6.1)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (0.37.1)\n",
      "Requirement already satisfied: absl-py>=0.4 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (1.1.0)\n",
      "Requirement already satisfied: protobuf<3.20,>=3.9.2 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (3.19.4)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (2.1.2)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (1.47.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (0.4.6)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from tensorboard->keras-tuner) (1.8.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (4.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (5.2.0)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from google-auth<3,>=1.6.3->tensorboard->keras-tuner) (1.16.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (1.3.1)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from jedi>=0.16->ipython->keras-tuner) (0.8.3)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from markdown>=2.6.8->tensorboard->keras-tuner) (4.12.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\hager\\anaconda3\\envs\\hager\\lib\\site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->keras-tuner) (3.8.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->keras-tuner) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\hager\\appdata\\roaming\\python\\python39\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->keras-tuner) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XjZSolvm8K25",
    "outputId": "5833e87b-1dea-46e0-8f4d-0059df1218d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hager\\AppData\\Local\\Temp/ipykernel_25924/3664098320.py:42: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  from kerastuner import HyperModel\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "le = LabelEncoder() \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from keras.models import Sequential\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gensim\n",
    "import re\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GlobalMaxPooling1D\n",
    "\n",
    "from keras import backend as K\n",
    "from kerastuner import HyperModel\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from kerastuner.tuners import RandomSearch\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IlzBnQ_z8PAU",
    "outputId": "db3bed47-bfe2-434f-a1f0-a25effa6a73d"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O6NVfV8DRPSF"
   },
   "outputs": [],
   "source": [
    "word2vemodel = gensim.models.Word2Vec.load('/content/drive/MyDrive/ArabicSentimentHager/full_grams_cbow_300_twitter.mdl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5zlMlH-QjFz"
   },
   "source": [
    "## Read training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqNfvg4O8K3B"
   },
   "outputs": [],
   "source": [
    "#read train dataset\n",
    "train= pd.read_csv('/content/drive/MyDrive/ArabicSentimentHager/paper2/Deep/sg/dataset1/Training_Main-AHS.csv',encoding='utf-8')\n",
    "train=train.dropna()\n",
    "\n",
    "y_train1 = train.label                           \n",
    "X_train= train.drop('label',axis = 1 )\n",
    "\n",
    "train.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlfPCQcq8K3E",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read testing dataset\n",
    "test= pd.read_csv('/content/drive/MyDrive/ArabicSentimentHager/paper2/Deep/sg/dataset1/Testing_Main-AHS.csv',encoding='utf-8')\n",
    "test=test.dropna()\n",
    "\n",
    "y_test1= test.label                           \n",
    "X_test= test.drop('label',axis = 1 )\n",
    "test.label.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Xd6cfsIQ1eS"
   },
   "source": [
    "## convert label to categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tusq7R1v8K3G"
   },
   "outputs": [],
   "source": [
    "#convert label to categorical\n",
    "encoder_train_y = LabelEncoder()\n",
    "y_train= encoder_train_y.fit_transform(y_train1)\n",
    "y_train_categorical = np_utils.to_categorical(y_train)\n",
    "\n",
    "encoder_test_y = LabelEncoder()\n",
    "y_test = encoder_test_y.fit_transform(y_test1)\n",
    "y_test_categorical=np_utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QrheAyuFTudw"
   },
   "source": [
    "## Convert x_train and X_test into number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_jZ2ZRUO8K3I"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 140 \n",
    "MAX_NUM_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "tokenizer.fit_on_texts(X_train['text'])\n",
    "train_sequences = tokenizer.texts_to_sequences(X_train['text'])\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "padded_train = pad_sequences(sequences=train_sequences, maxlen=MAX_LEN)\n",
    "\n",
    "test_sequences = tokenizer.texts_to_sequences(X_test['text'])\n",
    "padded_test = pad_sequences(sequences=test_sequences, maxlen=MAX_LEN)\n",
    "\n",
    "print('Total unique tokens generated: ',len(word_index))\n",
    "print('Shape of padded train tensor: ', padded_train.shape)\n",
    "print('Shape of padded test tensor: ', padded_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6kmemqxmT9nP"
   },
   "source": [
    "## Bulid embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LQ7S_508K3J"
   },
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word_index))\n",
    "word_embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))\n",
    "\n",
    "for word, index in word_index.items():\n",
    "    if index > MAX_NUM_WORDS:\n",
    "        continue\n",
    "    # embedding_vector = model[word]\n",
    "    #word = clean_str(word)\n",
    "    if word not in word2vemodel.wv:\n",
    "        embedding_vector = None\n",
    "    else:\n",
    "        embedding_vector = word2vemodel.wv[word]\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZlo8IktUIXx"
   },
   "source": [
    "## Define some of function that are used as matixs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-1WP2Ta8K3K"
   },
   "outputs": [],
   "source": [
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HslHUgQJUgJD"
   },
   "source": [
    "## Optimize and build mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RcTGP13Y8K3L"
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_model(hp):\n",
    "    num_units_min  =  50\n",
    "    num_units_max  =  1000\n",
    "    num_units_step =  50\n",
    "\n",
    "    dropout_min  =  .1\n",
    "    dropout_max  =  0.9\n",
    "    dropout_step =  0.1\n",
    "    model = keras.Sequential()\n",
    "    \n",
    "    model.add(layers.Embedding(num_words+1, EMBEDDING_DIM, weights=[word_embedding_matrix], input_length=MAX_LEN))\n",
    "    \n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            filters=hp.Choice(\n",
    "                'num_filters1',[128,256] ),\n",
    "            activation='relu',\n",
    "            kernel_size=hp.Choice('kernel_size1', [2, 3,4,5])))\n",
    "  \n",
    "    model.add(MaxPooling1D(pool_size=hp.Choice('Pool_Size1', [2, 3,4,5])))\n",
    "\n",
    "   \n",
    "\n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            filters=hp.Choice(\n",
    "                'num_filters2',[128,256] ),\n",
    "            activation='relu',\n",
    "            kernel_size=hp.Choice('kernel_size2', [4,5])))\n",
    "   \n",
    "\n",
    "    model.add(MaxPooling1D(pool_size=hp.Choice('Pool_Size2', [4,5])))\n",
    "\n",
    "  \n",
    "    model.add(\n",
    "        Conv1D(\n",
    "            filters=hp.Choice(\n",
    "                'num_filters3',[128,256,512] ),\n",
    "            activation='relu',\n",
    "            kernel_size=hp.Choice('kernel_size3', [2, 3,4,5])))    \n",
    "    model.add(GlobalMaxPooling1D()  )\n",
    "    \n",
    " \n",
    "\n",
    "    \n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(layers.Dense(units=hp.Int('unit1',  min_value=num_units_min,\n",
    "                                                 max_value=num_units_max,\n",
    "                                                 step=num_units_step),activation = 'relu'))\n",
    "    #model.add(Dropout(rate=hp.Float('dropout4', min_value=dropout_min, max_value=dropout_max , default=dropout_step)))\n",
    "\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "    \n",
    "    model.compile(optimizer=keras.optimizers.Adam( hp.Float('learning_rate',min_value=1e-4,max_value=1e-2)),loss='binary_crossentropy', metrics=['acc',f1_m,precision_m, recall_m] )\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Go5T198Y9Ee9"
   },
   "outputs": [],
   "source": [
    "\n",
    "tuner = RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_acc',\n",
    "    max_trials=10,\n",
    "    project_name='/content/drive/MyDrive/ArabicSentimentHager/paper2/Deep/CBOW/dataset1/CNN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B3yFp2dj8K3M"
   },
   "outputs": [],
   "source": [
    "epoch=20\n",
    "BATCH_SIZE=1000\n",
    "early_stopping = EarlyStopping(monitor='val_acc', patience=30, verbose=1)\n",
    "callback_list = [ early_stopping ]\n",
    "\n",
    "# split training data into stratified train/dev sets\n",
    "\n",
    "h=tuner.search(padded_train, y_train_categorical,\n",
    "             epochs=epoch,\n",
    "             batch_size=BATCH_SIZE, \n",
    "             callbacks=callback_list, validation_data=(padded_test,y_test_categorical) )\n",
    "            \n",
    "model = tuner.get_best_models(num_models=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVbe0mWO1FqH"
   },
   "outputs": [],
   "source": [
    "model.save(\"/content/drive/MyDrive/ArabicSentimentHager/paper2/Deep/CBOW/dataset1/model/CNN.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z0yYtiEa8K3N"
   },
   "outputs": [],
   "source": [
    "history=model.fit(padded_train,\n",
    "                  y_train_categorical,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  epochs=epoch, validation_data=(padded_test,y_test_categorical))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbciBUjb8K3N",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tLJa080e8K3N"
   },
   "outputs": [],
   "source": [
    "tuner.get_best_hyperparameters()[0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cZ85jUmqQYRx"
   },
   "outputs": [],
   "source": [
    "AccuracyTest=[]\n",
    "PrecisionTest=[]\n",
    "RecallTest=[]\n",
    "F1Test=[]\n",
    "for i in range(0,1): \n",
    "    model.fit(padded_train, y_train_categorical, epochs=epoch, batch_size=BATCH_SIZE,  verbose=1, shuffle=True)\n",
    "    y_p = model.predict(padded_test)\n",
    "    y_pred= np.argmax(y_p, axis=1)\n",
    "\n",
    "    Accurcy_Test= accuracy_score(y_test,y_pred)\n",
    "    Precision_Test=precision_score(y_test, y_pred,average='weighted')\n",
    "\n",
    "    Recall_Test=recall_score(y_test, y_pred, average='weighted')\n",
    "    F1_Test=f1_score(y_test, y_pred, average='weighted') \n",
    "\n",
    "    AccuracyTest.append(round(100*Accurcy_Test, 2))\n",
    "    PrecisionTest.append(round(100*Precision_Test, 2))\n",
    "    RecallTest.append(round(100*Recall_Test, 2))\n",
    "    F1Test.append(round(100*F1_Test, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k25fKcEyvQ3q"
   },
   "outputs": [],
   "source": [
    "ReultofTest=pd.DataFrame([])\n",
    "ReultofTest=ReultofTest.append({'AccuracyTest' : round(np.mean(AccuracyTest),2),'PrecisionTest':round(np.mean(PrecisionTest),2),\n",
    "             'RecallTest' : round(np.mean(RecallTest),2),'F1Test':round(np.mean(F1Test),2)}, ignore_index=True)\n",
    "\n",
    "ReultofTest.reindex(['AccuracyTest','PrecisionTest','RecallTest','F1Test'], axis=1)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "CNN_GRU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
